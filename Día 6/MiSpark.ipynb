{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminares\n",
    "\n",
    "1. Realizar la descarga del contenedor en caso de no haberlo hecho:\n",
    "\n",
    "* `docker pull jdvelasq/pyspark:2.4.7-pseudo`\n",
    "\n",
    "2. Correr el contenedor en una terminal de Visual Studio Code\n",
    "* `docker run --rm -it -v C:\\dataspark:/datalake -p 8088:8088 jdvelasq/pyspark:2.4.7-pseudo`\n",
    "\n",
    "3. Recordar hacer el Attach del contenedor en Visual Studio Code. Si no aparece la opción debe instalar la extensión Remote - Containers\n",
    "\n",
    "4. Ejecutar el IDE (Integrated development environment) dentro de la terminal del contenedor\n",
    "* `jupyter lab --ip=0.0.0.0`\n",
    "\n",
    "* También puede usar el editor de Visual Studio Code\n",
    "\n",
    "* O usar la consola interactiva de pyspark: `pyspark --jars /datalake/flint-0.6.0.jar --py-files /datalake/flint-0.6.0.jar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación de las librerias para trabajar Series de Tiempo en pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: pyarrow==0.9.0 in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: six>=1.0.0 in /usr/lib/python3/dist-packages (from pyarrow==0.9.0)\n",
      "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from pyarrow==0.9.0)\n",
      "Requirement already satisfied: ts-flint in /usr/local/lib/python3.6/dist-packages\n",
      "--2021-04-24 02:28:56--  https://repo1.maven.org/maven2/com/twosigma/flint/0.6.0/flint-0.6.0.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.4.209, 199.232.32.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.4.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2171677 (2.1M) [application/java-archive]\n",
      "Saving to: 'flint-0.6.0.jar.4'\n",
      "\n",
      "flint-0.6.0.jar.4   100%[===================>]   2.07M  3.95MB/s    in 0.5s    \n",
      "\n",
      "2021-04-24 02:28:57 (3.95 MB/s) - 'flint-0.6.0.jar.4' saved [2171677/2171677]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Es una libreria que da soporte para llamar funciones, clases y tipos del Lenguaje C\n",
    "# Es un requerimiento para ts.flint\n",
    "!pip3 install Cython\n",
    "\n",
    "# Es una libreria especial para la serialización de arreglos basada en C++, permite la \n",
    "# integración entre pandas, NumPy, Spark ... \n",
    "# https://pypi.org/project/pyarrow/\n",
    "# Es un requerimiento para ts.flint\n",
    "!pip3 install pyarrow==0.9.0\n",
    "\n",
    "# Es una colección de módulos para el análisis de series de tiempo con PySpark\n",
    "# https://ts-flint.readthedocs.io/en/latest/\n",
    "!pip3 install ts-flint\n",
    "\n",
    "# Descargamos las librerias java de flint\n",
    "# quedará en la carpeta /datalake\n",
    "!wget https://repo1.maven.org/maven2/com/twosigma/flint/0.6.0/flint-0.6.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Requerido https://github.com/twosigma/flint/blob/master/python/README.md\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /datalake/flint-0.6.0.jar --py-files /datalake/flint-0.6.0.jar pyspark-shell'\n",
    "\n",
    "# La libreria para \"encontrar el sevicio\" de Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Librerias para \"gestionar el servicio\" de Spark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "\n",
    "# Creamos una aplicación Spark en el Servicio\n",
    "# Tenga cuidado con las tildes o caracteres especiales en el nombre de la app\n",
    "AppSpark = SparkConf().setAppName(\"Valores y Acciones SA\")\n",
    "\n",
    "# definimos un espacio o contexto para la App\n",
    "ContextoSpark=SparkContext(conf=AppSpark)\n",
    "\n",
    "# inicio una sesión en el espacio de la App\n",
    "SesionSpark = SparkSession(ContextoSpark)\n",
    "\n",
    "# inicio del espacio o contexto SQL\n",
    "ContextoSql = SQLContext(sparkContext=ContextoSpark, sparkSession=SesionSpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consultamos las tablas en SQL Spark\n",
    "ContextoSql.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `sp500.csv': File exists\n",
      "Found 1 items\n",
      "-rw-r--r--   1 root supergroup    1246208 2021-04-24 02:00 sp500.csv\n"
     ]
    }
   ],
   "source": [
    "# Ahora cargamos los datos en el file system (FS) de hadoop\n",
    "# recordar que el archivo local esta en la carpeta /datalake\n",
    "\n",
    "!hadoop fs -put sp500.csv\n",
    "!hadoop fs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una tabla a partir de datos CSV\n",
    "# Se toma el CSV desde el FS de Hadoop\n",
    "#https://www.learningjournal.guru/courses/spark/spark-foundation-training/spark-sql-database-and-table/\n",
    "#https://spark.apache.org/docs/latest/sql-ref-syntax-ddl.html\n",
    "\n",
    "ContextoSql.sql(\"\"\"\n",
    "CREATE TABLE \n",
    "    sp500_crudo \n",
    "USING com.databricks.spark.csv \n",
    "OPTIONS (\n",
    "    path 'sp500.csv', \n",
    "    header 'true')\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+\n",
      "|database|  tableName|isTemporary|\n",
      "+--------+-----------+-----------+\n",
      "| default|sp500_crudo|      false|\n",
      "+--------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ContextoSql.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+-----+-----+-----+---------+-------+\n",
      "| id|               Date| Open| High|  Low|Close|Adj Close| Volume|\n",
      "+---+-------------------+-----+-----+-----+-----+---------+-------+\n",
      "|  1|1950-01-03 00:00:00|16.66|16.66|16.66|16.66|    16.66|1260000|\n",
      "|  2|1950-01-04 00:00:00|16.85|16.85|16.85|16.85|    16.85|1890000|\n",
      "|  3|1950-01-05 00:00:00|16.93|16.93|16.93|16.93|    16.93|2550000|\n",
      "|  4|1950-01-06 00:00:00|16.98|16.98|16.98|16.98|    16.98|2010000|\n",
      "|  5|1950-01-09 00:00:00|17.08|17.08|17.08|17.08|    17.08|2520000|\n",
      "|  6|1950-01-10 00:00:00|17.03|17.03|17.03|17.03|    17.03|2160000|\n",
      "|  7|1950-01-11 00:00:00|17.09|17.09|17.09|17.09|    17.09|2630000|\n",
      "|  8|1950-01-12 00:00:00|16.76|16.76|16.76|16.76|    16.76|2970000|\n",
      "|  9|1950-01-13 00:00:00|16.67|16.67|16.67|16.67|    16.67|3330000|\n",
      "| 10|1950-01-16 00:00:00|16.72|16.72|16.72|16.72|    16.72|1460000|\n",
      "+---+-------------------+-----+-----+-----+-----+---------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consulta = ContextoSql.sql(\"Select * from sp500_crudo\")\n",
    "consulta.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|               date|close|\n",
      "+-------------------+-----+\n",
      "|1950-01-03 00:00:00|16.66|\n",
      "|1950-01-04 00:00:00|16.85|\n",
      "|1950-01-05 00:00:00|16.93|\n",
      "|1950-01-06 00:00:00|16.98|\n",
      "|1950-01-09 00:00:00|17.08|\n",
      "|1950-01-10 00:00:00|17.03|\n",
      "|1950-01-11 00:00:00|17.09|\n",
      "|1950-01-12 00:00:00|16.76|\n",
      "|1950-01-13 00:00:00|16.67|\n",
      "|1950-01-16 00:00:00|16.72|\n",
      "+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "Tipo de dato: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- close: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consulta = ContextoSql.sql(\"Select date, close from sp500_crudo\")\n",
    "consulta.show(10)\n",
    "print(\"Tipo de dato:\", type(consulta))\n",
    "consulta.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+\n",
      "| col_name|data_type|comment|\n",
      "+---------+---------+-------+\n",
      "|       id|   string|   null|\n",
      "|     Date|   string|   null|\n",
      "|     Open|   string|   null|\n",
      "|     High|   string|   null|\n",
      "|      Low|   string|   null|\n",
      "|    Close|   string|   null|\n",
      "|Adj Close|   string|   null|\n",
      "|   Volume|   string|   null|\n",
      "+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consulta = ContextoSql.sql(\"describe sp500_crudo\")\n",
    "consulta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo de variable <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSp500Crudo = ContextoSql.table('sp500_crudo')\n",
    "\n",
    "#Mostramos el tipo de dato\n",
    "print(\"Tipo de variable\", type(dfSp500Crudo))\n",
    "\n",
    "#Mostramos la estructura\n",
    "dfSp500Crudo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos con sus correspondientes tipos\n",
    "# Recordar que el CSV debe estar en el File System de Hadoop\n",
    "ContextoSql.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS \n",
    "sp500 (\n",
    "        id LONG,\n",
    "        date TIMESTAMP,\n",
    "        open FLOAT,\n",
    "        high FLOAT,\n",
    "        low FLOAT,\n",
    "        close FLOAT,\n",
    "        adj_close FLOAT,\n",
    "        volume BIGINT\n",
    "        ) \n",
    "USING CSV OPTIONS ( \n",
    "    header='true', \n",
    "    nullvalue='NA', \n",
    "    timestampFormat=\\\"yyyy-MM-dd'T'HH:mm:ss\\\", \n",
    "    path='sp500.csv')\n",
    "    \"\"\"); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+\n",
      "|database|  tableName|isTemporary|\n",
      "+--------+-----------+-----------+\n",
      "| default|      sp500|      false|\n",
      "| default|sp500_crudo|      false|\n",
      "+--------+-----------+-----------+\n",
      "\n",
      "+---+-------------------+-----+-----+-----+-----+---------+-------+\n",
      "| id|               Date| Open| High|  Low|Close|Adj Close| Volume|\n",
      "+---+-------------------+-----+-----+-----+-----+---------+-------+\n",
      "|  1|1950-01-03 00:00:00|16.66|16.66|16.66|16.66|    16.66|1260000|\n",
      "|  2|1950-01-04 00:00:00|16.85|16.85|16.85|16.85|    16.85|1890000|\n",
      "|  3|1950-01-05 00:00:00|16.93|16.93|16.93|16.93|    16.93|2550000|\n",
      "|  4|1950-01-06 00:00:00|16.98|16.98|16.98|16.98|    16.98|2010000|\n",
      "|  5|1950-01-09 00:00:00|17.08|17.08|17.08|17.08|    17.08|2520000|\n",
      "|  6|1950-01-10 00:00:00|17.03|17.03|17.03|17.03|    17.03|2160000|\n",
      "|  7|1950-01-11 00:00:00|17.09|17.09|17.09|17.09|    17.09|2630000|\n",
      "|  8|1950-01-12 00:00:00|16.76|16.76|16.76|16.76|    16.76|2970000|\n",
      "|  9|1950-01-13 00:00:00|16.67|16.67|16.67|16.67|    16.67|3330000|\n",
      "| 10|1950-01-16 00:00:00|16.72|16.72|16.72|16.72|    16.72|1460000|\n",
      "+---+-------------------+-----+-----+-----+-----+---------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+---------+-------+\n",
      "| col_name|data_type|comment|\n",
      "+---------+---------+-------+\n",
      "|       id|   bigint|   null|\n",
      "|     date|timestamp|   null|\n",
      "|     open|    float|   null|\n",
      "|     high|    float|   null|\n",
      "|      low|    float|   null|\n",
      "|    close|    float|   null|\n",
      "|adj_close|    float|   null|\n",
      "|   volume|   bigint|   null|\n",
      "+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ContextoSql.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "consulta = ContextoSql.sql(\"Select * from sp500_crudo\")\n",
    "consulta.show(10)\n",
    "\n",
    "ContextoSql.sql(\"describe sp500\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------+-------+-------+-------+---------+----------+\n",
      "|   id|               Date|   Open|   High|    Low|  Close|Adj Close|    Volume|\n",
      "+-----+-------------------+-------+-------+-------+-------+---------+----------+\n",
      "|17356|2018-12-21 00:00:00|2465.38|2504.41|2408.55|2416.62|  2416.62|7609010000|\n",
      "|17355|2018-12-20 00:00:00|2496.77|2509.63|2441.18|2467.42|  2467.42|5585780000|\n",
      "|17354|2018-12-19 00:00:00|2547.05|2585.29|2488.96|2506.96|  2506.96|5127940000|\n",
      "|17353|2018-12-18 00:00:00| 2559.9|2573.99|2528.71|2546.16|  2546.16|4470880000|\n",
      "|17352|2018-12-17 00:00:00|2590.75|2601.13|2530.54|2545.94|  2545.94|4616350000|\n",
      "|17351|2018-12-14 00:00:00|2629.68|2635.07|2593.84|2599.95|  2599.95|4035020000|\n",
      "|17350|2018-12-13 00:00:00| 2658.7|2670.19|2637.27|2650.54|  2650.54|3927720000|\n",
      "|17349|2018-12-12 00:00:00|2658.23|2685.44|2650.26|2651.07|  2651.07|3955890000|\n",
      "|17348|2018-12-11 00:00:00|2664.44|2674.35| 2621.3|2636.78|  2636.78|3905870000|\n",
      "|17347|2018-12-10 00:00:00|2630.86|2647.51|2583.23|2637.72|  2637.72|4151030000|\n",
      "+-----+-------------------+-------+-------+-------+-------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consulta = ContextoSql.sql(\"Select * from sp500_crudo ORDER By date DESC\")\n",
    "consulta.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------+-------+-------+-------+---------+----------+\n",
      "|   id|               Date|   Open|   High|    Low|  Close|Adj Close|    Volume|\n",
      "+-----+-------------------+-------+-------+-------+-------+---------+----------+\n",
      "|16927|2017-04-10 00:00:00|2357.16|2366.37| 2351.5|2357.16|  2357.16|2785410000|\n",
      "|15292|2010-10-11 00:00:00|1165.32|1168.68|1162.02|1165.32|  1165.32|2505900000|\n",
      "|14207|2006-06-20 00:00:00|1240.12|1249.01|1238.87|1240.12|  1240.12|2232950000|\n",
      "|11844|1997-01-28 00:00:00| 765.02| 776.32| 761.75| 765.02|   765.02| 541580000|\n",
      "|11327|1995-01-12 00:00:00| 461.64| 461.93| 460.63| 461.64|   461.64| 313040000|\n",
      "|11142|1994-04-19 00:00:00| 442.54| 444.82| 438.83| 442.54|   442.54| 323280000|\n",
      "|10732|1992-09-03 00:00:00| 417.98| 420.31| 417.49| 417.98|   417.98| 212500000|\n",
      "| 9784|1988-12-05 00:00:00| 274.93| 275.62| 271.81| 274.93|   274.93| 144660000|\n",
      "| 9280|1986-12-08 00:00:00| 251.16| 252.36| 248.82| 251.16|   251.16| 159000000|\n",
      "| 9229|1986-09-25 00:00:00| 231.83| 236.28| 230.67| 231.83|   231.83| 134300000|\n",
      "+-----+-------------------+-------+-------+-------+-------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Se encontraron:  3104  registros.\n"
     ]
    }
   ],
   "source": [
    "consulta = ContextoSql.sql(\"Select * from sp500_crudo where open = close ORDER By date DESC\")\n",
    "consulta.show(10)\n",
    "print(\"Se encontraron: \", consulta.count(), \" registros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------+-------+-------+-------+---------+----------+\n",
      "|   id|               Date|   Open|   High|    Low|  Close|Adj Close|    Volume|\n",
      "+-----+-------------------+-------+-------+-------+-------+---------+----------+\n",
      "|16860|2017-01-03 00:00:00|2251.57|2263.88|2245.13|2257.83|  2257.83|3770530000|\n",
      "|16861|2017-01-04 00:00:00| 2261.6|2272.82| 2261.6|2270.75|  2270.75|3764890000|\n",
      "|16862|2017-01-05 00:00:00|2268.18| 2271.5|2260.45|   2269|     2269|3761820000|\n",
      "|16863|2017-01-06 00:00:00|2271.14| 2282.1|2264.06|2276.98|  2276.98|3339890000|\n",
      "|16864|2017-01-09 00:00:00|2273.59|2275.49| 2268.9| 2268.9|   2268.9|3217610000|\n",
      "|16865|2017-01-10 00:00:00|2269.72|2279.27|2265.27| 2268.9|   2268.9|3638790000|\n",
      "|16866|2017-01-11 00:00:00| 2268.6|2275.32|2260.83|2275.32|  2275.32|3620410000|\n",
      "|16867|2017-01-12 00:00:00|2271.14|2271.78|2254.25|2270.44|  2270.44|3462130000|\n",
      "|16868|2017-01-13 00:00:00|2272.74|2278.68|2271.51|2274.64|  2274.64|3081270000|\n",
      "|16869|2017-01-17 00:00:00|2269.14|2272.08|2262.81|2267.89|  2267.89|3584990000|\n",
      "+-----+-------------------+-------+-------+-------+-------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Se encontraron:  497  registros.\n"
     ]
    }
   ],
   "source": [
    "consulta = ContextoSql.sql(\"Select * from sp500_crudo where date > '2017-00-00'\")\n",
    "consulta.show(10)\n",
    "print(\"Se encontraron: \", consulta.count(), \" registros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------+-------+-------+-------+---------+----------+\n",
      "|   id|               Date|   Open|   High|    Low|  Close|Adj Close|    Volume|\n",
      "+-----+-------------------+-------+-------+-------+-------+---------+----------+\n",
      "|16860|2017-01-03 00:00:00|2251.57|2263.88|2245.13|2257.83|  2257.83|3770530000|\n",
      "|16861|2017-01-04 00:00:00| 2261.6|2272.82| 2261.6|2270.75|  2270.75|3764890000|\n",
      "|16862|2017-01-05 00:00:00|2268.18| 2271.5|2260.45|   2269|     2269|3761820000|\n",
      "|16863|2017-01-06 00:00:00|2271.14| 2282.1|2264.06|2276.98|  2276.98|3339890000|\n",
      "|16864|2017-01-09 00:00:00|2273.59|2275.49| 2268.9| 2268.9|   2268.9|3217610000|\n",
      "|16865|2017-01-10 00:00:00|2269.72|2279.27|2265.27| 2268.9|   2268.9|3638790000|\n",
      "|16866|2017-01-11 00:00:00| 2268.6|2275.32|2260.83|2275.32|  2275.32|3620410000|\n",
      "|16867|2017-01-12 00:00:00|2271.14|2271.78|2254.25|2270.44|  2270.44|3462130000|\n",
      "|16868|2017-01-13 00:00:00|2272.74|2278.68|2271.51|2274.64|  2274.64|3081270000|\n",
      "|16869|2017-01-17 00:00:00|2269.14|2272.08|2262.81|2267.89|  2267.89|3584990000|\n",
      "+-----+-------------------+-------+-------+-------+-------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Se encontraron:  251  registros.\n"
     ]
    }
   ],
   "source": [
    "consulta = ContextoSql.sql(\"Select * from sp500_crudo where date > '2017-00-00' and date < '2018-00-00'\")\n",
    "consulta.show(10)\n",
    "print(\"Se encontraron: \", consulta.count(), \" registros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|close|\n",
      "+---+-----+\n",
      "|  1|16.66|\n",
      "|  2|16.85|\n",
      "|  3|16.93|\n",
      "|  4|16.98|\n",
      "|  5|17.08|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+-------------------+-------+\n",
      "|   id|               date|   open|\n",
      "+-----+-------------------+-------+\n",
      "|17038|2017-09-18 00:00:00|2502.51|\n",
      "|17039|2017-09-19 00:00:00|2506.29|\n",
      "|17040|2017-09-20 00:00:00|2506.84|\n",
      "|17041|2017-09-21 00:00:00|2507.16|\n",
      "|17044|2017-09-26 00:00:00|2501.04|\n",
      "+-----+-------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[Row(id=1, date=datetime.datetime(1950, 1, 3, 0, 0), open=16.65999984741211, high=16.65999984741211, low=16.65999984741211, close=16.65999984741211, adj_close=16.65999984741211, volume=1260000), Row(id=2, date=datetime.datetime(1950, 1, 4, 0, 0), open=16.850000381469727, high=16.850000381469727, low=16.850000381469727, close=16.850000381469727, adj_close=16.850000381469727, volume=1890000), Row(id=3, date=datetime.datetime(1950, 1, 5, 0, 0), open=16.93000030517578, high=16.93000030517578, low=16.93000030517578, close=16.93000030517578, adj_close=16.93000030517578, volume=2550000)]\n"
     ]
    }
   ],
   "source": [
    "miTabla = ContextoSql.table('sp500')\n",
    "\n",
    "consulta = miTabla.select(['id', 'close']).show(5)\n",
    "\n",
    "consulta = miTabla.select(['id', 'date', 'open']).filter(miTabla['open'] > 2500).show(5)\n",
    "\n",
    "print(miTabla.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|        promOpen|        promClose|\n",
      "+----------------+-----------------+\n",
      "|568.127855878295|568.2315907784941|\n",
      "+----------------+-----------------+\n",
      "\n",
      "+-------+--------+\n",
      "|maxOpen|maxClose|\n",
      "+-------+--------+\n",
      "|2936.76| 2930.75|\n",
      "+-------+--------+\n",
      "\n",
      "+-------+--------+\n",
      "|minOpen|minClose|\n",
      "+-------+--------+\n",
      "|  16.66|   16.66|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#agrupaciones \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "miTabla.agg(F.avg('open').alias('promOpen'), F.avg('close').alias('promClose')).show()\n",
    "                                                                  \n",
    "miTabla.agg(F.max('open').alias('maxOpen'), F.max('close').alias('maxClose')).show()\n",
    "\n",
    "miTabla.agg(F.min('open').alias('minOpen'), F.min('close').alias('minClose')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|maxOpen|maxClose|\n",
      "+-------+--------+\n",
      "|2692.71| 2690.16|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mi2017 = miTabla.filter((miTabla['date'] > '2017-00-00') & (miTabla['date'] < '2018-00-00'))\n",
    "\n",
    "mi2017.agg(F.max('open').alias('maxOpen'), F.max('close').alias('maxClose')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+--------+------+\n",
      "|year|           avgOpen|avgClose|mxOpen|\n",
      "+----+------------------+--------+------+\n",
      "|1950|18.397269145551935|    18.0| 20.43|\n",
      "|1951|22.321887594629004|    22.0| 23.85|\n",
      "|1952|24.496160057067872|    24.0| 26.59|\n",
      "|1953|24.722589675173815|    25.0| 26.66|\n",
      "|1954|29.724087359413268|    30.0| 35.98|\n",
      "|1955| 40.49884920271616|    40.0| 46.41|\n",
      "|1956|46.639521959768345|    47.0| 49.64|\n",
      "|1957| 44.42337299528576|    44.0| 49.13|\n",
      "|1958|46.203452367631215|    46.0| 55.21|\n",
      "|1959| 57.41818169454341|    57.0| 60.71|\n",
      "+----+------------------+--------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Se encontraron:  69  registros.\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Group by por año\n",
    "\n",
    "precioPromAnual = ContextoSql.sql(\"\"\" Select year(date) as year, avg(open) as avgOpen, round(avg(close)) as avgClose,\n",
    "                                      max(open) as mxOpen\n",
    "                                  from sp500 group by year(date) \n",
    "                                  order by year(date)\"\"\")\n",
    "precioPromAnual.show(10)\n",
    "print(\"Se encontraron: \", precioPromAnual.count(), \" registros.\")\n",
    "\n",
    "print(type(precioPromAnual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpia en caso de haberlo creado\n",
    "#!hadoop fs -rm precioPromAnual_csv/* \n",
    "#!hadoop fs -rmdir precioPromAnual_csv\n",
    "\n",
    "precioPromAnual.write.csv('precioPromAnual_csv', header=True)\n",
    "# Más opciones en: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se consulta el catalogo del resultado, se consolida y almacena\n",
    "!hadoop fs -cat 'precioPromAnual_csv/*' > precioPromAnual.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|preciospromediosa...|      false|\n",
      "| default|               sp500|      false|\n",
      "| default|         sp500_crudo|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Almacenar una consulta en una tabla\n",
    "precioPromAnual.write.format(\"orc\").saveAsTable(\"preciosPromediosAnuales\")\n",
    "ContextoSql.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+--------+-------+\n",
      "|year|           avgOpen|avgClose| mxOpen|\n",
      "+----+------------------+--------+-------+\n",
      "|2018|2754.1532508415903|  2752.0|2936.76|\n",
      "|2017| 2448.275887660296|  2449.0|2692.71|\n",
      "|2016| 2094.091549827939|  2095.0|2270.54|\n",
      "|2015|2061.2680160280256|  2061.0|2130.36|\n",
      "|2014|1930.7544851151724|  1931.0|2088.49|\n",
      "|2013|1642.2986488947793|  1644.0|1842.97|\n",
      "|2012|1378.6806381835938|  1379.0|1465.42|\n",
      "|2011|1267.6182105654761|  1268.0|1365.21|\n",
      "|2010|1139.3697606646826|  1140.0|1259.44|\n",
      "|2009| 947.0220634823754|   948.0|1128.55|\n",
      "+----+------------------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+------------------+--------+------+\n",
      "|year|           avgOpen|avgClose|mxOpen|\n",
      "+----+------------------+--------+------+\n",
      "|1950|18.397269145551935|    18.0| 20.43|\n",
      "|1951|22.321887594629004|    22.0| 23.85|\n",
      "|1952|24.496160057067872|    24.0| 26.59|\n",
      "|1953|24.722589675173815|    25.0| 26.66|\n",
      "|1954|29.724087359413268|    30.0| 35.98|\n",
      "|1955| 40.49884920271616|    40.0| 46.41|\n",
      "|1956|46.639521959768345|    47.0| 49.64|\n",
      "|1957| 44.42337299528576|    44.0| 49.13|\n",
      "|1958|46.203452367631215|    46.0| 55.21|\n",
      "|1959| 57.41818169454341|    57.0| 60.71|\n",
      "+----+------------------+--------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Uso de la tabla creada\n",
    "promedios = ContextoSql.table('preciosPromediosAnuales')\n",
    "promedios.orderBy(F.col(\"year\").desc()).show(10)\n",
    "promedios.orderBy(F.col(\"year\").asc()).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ts.flint.dataframe.TimeSeriesDataFrame'>\n",
      "+-------------------+-----------+\n",
      "|               time|rendimiento|\n",
      "+-------------------+-----------+\n",
      "|1950-01-03 00:00:00|        0.0|\n",
      "|1950-01-04 00:00:00|        0.0|\n",
      "|1950-01-05 00:00:00|        0.0|\n",
      "|1950-01-06 00:00:00|        0.0|\n",
      "|1950-01-09 00:00:00|        0.0|\n",
      "+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+-------------------+\n",
      "|               time|        rendimiento|\n",
      "+-------------------+-------------------+\n",
      "|1962-01-02 00:00:00| -82.46036756299341|\n",
      "|1962-01-03 00:00:00|  23.95690123148708|\n",
      "|1962-01-04 00:00:00| -68.88765225405015|\n",
      "|1962-01-05 00:00:00|-138.73098974978308|\n",
      "|1962-01-08 00:00:00| -77.51950895002626|\n",
      "+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ahora usaremos la librería ts.flint\n",
    "import ts.flint\n",
    "from ts.flint import FlintContext\n",
    "\n",
    "# Creamos un espacio o contexto de Flint\n",
    "# Indicamos a Flint Cual es la Sesión de Spark\n",
    "#\n",
    "ContextoFlint = FlintContext(ContextoSql)\n",
    "\n",
    "# Usamos el contexto de la libreria Flint\n",
    "tSerie = ContextoFlint.read.dataframe(ContextoSql.table('sp500').withColumnRenamed('Date', 'time'))\n",
    "\n",
    "print(type(tSerie))\n",
    "\n",
    "tSerie_t1 = tSerie.withColumn('rendimiento', 10000 * (tSerie['Close'] - tSerie['Open']) / tSerie['Open']).select('time', 'rendimiento')\n",
    "tSerie_t1.show(5)\n",
    "\n",
    "# En algunos días el precio de cierre es igual al de apertura\n",
    "# Se muestran los diferentes de cero\n",
    "tSerie_t1.filter(tSerie_t1['rendimiento'] != 0).show(5)\n",
    "\n",
    "# Más información y ejemplos en\n",
    "# https://github.com/twosigma/flint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otros recomendados\n",
    "* Advanced Analytics with Spark: Patterns for Learning from Data at Scale por Sandy Ryza, Uri Laserson, Sean Owen, Josh Wills\n",
    "* [How to run pyspark in a Jupyter Notebook](https://www.hackdeploy.com/how-to-run-pyspark-in-a-jupyter-notebook/)\n",
    "* [pyts: A Python Package for Time Series Classification] (https://hal.inria.fr/hal-02883389/document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
